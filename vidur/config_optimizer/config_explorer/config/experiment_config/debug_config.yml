clusters:
  - device: a100
    num_gpus: 16
    gpus_per_node: 4


load_balancers:
#  - name: round_robin
#    identifier: round_robin
  - name: optimal
    identifier: opt
    target_metric: "min_latency"
  - name: optimal
    identifier: opt
    target_metric: "max_avg_batch_size"
#  - name: least_of_requests
#    identifier: lor
#  - name: random
#    identifier: random

schedulers:
  - scheduler: vllm

traces:
  - name: arxiv
    trace_file: "./data/processed_traces/arxiv_summarization_stats_llama2_tokenizer_filtered_v2.csv"
    max_seq_len: 4096
    num_requests: 160
    start_qps: 16
    generate_type: "synthetic"
#  - name: chat
#    trace_file: "./data/processed_traces/splitwise_code.csv"
#    max_seq_len: 4096
#    num_requests: 16000
#    start_qps: 32
#    type: "trace_replay"
#  - name: code
#    trace_file: "./data/processed_traces/splitwise_conv.csv"
#    max_seq_len: 4096
#    num_requests: 16000
#    start_qps: 8
#    type: "trace_replay"

#batch_sizes: [32, 64, 128]
batch_sizes: [16]
tp_dimensions: [1]
pp_dimensions: [1]

models:
  # - name: phi-2
  #   identifier: microsoft/phi-2
  #   exclude_tp_dims: [2, 4, 8]
  - name: llama-2-7b-hf
    identifier: meta-llama/Llama-2-7b-hf
  # - name: internlm-20b
  #   identifier: internlm/internlm-20b
  #   exclude_tp_dims: [1]
  # - name: llama-2-70b-hf
  #   identifier: meta-llama/Llama-2-70b-hf
  #   exclude_tp_dims: [1]
  # - name: qwen-72b
  #   identifier: Qwen/Qwen-72B
  #   exclude_tp_dims: [1]
